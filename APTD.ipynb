{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"APTD.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HhHGtsrg6QSp"},"source":["# Overview\n","\n","This notebook provides an overview/abbreviated tutorial of the APTD package."]},{"cell_type":"markdown","metadata":{"id":"_9u81fSejaUa"},"source":["### Experiment and design\n","\n","We will use the example of an cognitive experiment, a variant of the *extended Stroop task*.  In this experiment, subjects are presented with stimuli (words on a screen) that vary along two *feature dimensions*: the color of the lettering that spells the word (color), and the sequence of letters itself (word).  Following the presentation of a stimulus, the subject must respond in a prescribed manner.  Like stimuli, responses vary along two two dimensions: manual (pointing) and vocal (speaking).\n","\n","Each subject selects a response to each stimulus based on which task or tasks they have been assigned.  In this context, a **task** means a mapping from a single simulus dimension to a single response dimension; it is assumed that concrete mappings between the levels of each feature dimension and the levels of each response dimension (e.g., mapping colors to verbal responses) has been specified *a priori*.  Thus, once the experiment begins, the expermentor can specify a task simply by naming an feature dimension and a response dimension (for example, the experimenter may specify \"point/color,\" and from this the subject will know which of several sets of instructions to follow).\n","\n","\\\n","\n","**Summary of stimulus and response variables**\n","\n","\\\n","\n","input dimension | name  | symbol  | number of feature values | feature 0 | feature 1 \n","------------- |------------- | ------------- | ------------- | ------------- |------------- \n","0 | color | C | 2 | red (R) | green (G)\n","1 | word | W | 2 | red (R) | green (G)\n","\n","output dimension | name  | symbol  | number of feature values | feature 0 | feature 1 \n","------------- |------------- | ------------- | ------------- | ------------- |------------- \n","0 | point | P | 2 | red (R) | green (G)\n","1 | speak | S | 2 | red (R) | green (G)\n","\n","\\\n","\n","## Experimental data\n","\n","\n","The demonstration will use the following (synthetic) data. Rows of the table are ordered such that stimuli follow lexicographic order\n","  * concretely, this means RR, RG, GR, GG\n","  * if the color dimension had taken three different feature values, RGB, then the order would be RR, RG, RB, GR, GG, GB\n","  * thus the entries in the column labeled `input (numpy code)` are identical for every task\n","\n","\\\n","\n","task | stimulus (dim 0: color)  | stimulus (dim 1: word)  | stimulus (vectorized) | instruction (dim 0: speak) | intruction (dim 1: point) | instruction (vectorized) | subject fmri response (vectorized) | subject behavior response (vectorized)\n","------------- |------------- | ------------- | ------------- | ------------- |------------- | ------------- | ------------- | ------------- \n","name color | R | R | [1, 0, 1, 0] |   R | - | [1, 0, 0, 0] |[1, 0, 0, 0] |[1, 0, 0, 0] |\n","\" | R | G | [1, 0, 0, 1] |   R | - | [1, 0, 0, 0] |[1, 0, 0, 0] |[1, 0, 0, 0] |\n","\" | G | R | [0, 1, 1, 0] |   G | - | [0, 1, 0, 0] |[0, 1, 0, 0] |[0, 1, 0, 0] |\n","\" | G | G | [0, 1, 0, 1] |   G | - | [0, 1, 0, 0] |[0, 1, 0, 0] |[0, 1, 0, 0] |\n","point color | R | R | [1, 0, 1, 0] |   - | R | [0, 0, 1, 0] |[0, 0, 1, 0] |[0, 0, 1, 0] |\n","\" | R | G | [1, 0, 0, 1] |   - | R | [0, 0, 1, 0] |[0, 0, 1, 0] |[0, 0, 1, 0] |\n","\" | G | R | [0, 1, 1, 0] |   - | G | [0, 0, 0, 1] |[0, 0, 0, 1] |[0, 0, 0, 1] |\n","\" | G | G | [0, 1, 0, 1] |   - | G | [0, 0, 0, 1] |[0, 0, 0, 1] |[0, 0, 0, 1] |\n","read color | R | R | [1, 0, 1, 0] |   R | - | [1, 0, 0, 0] |[1, 0, 0, 0] |[1, 0, 0, 0] |\n","\" | R | G | [1, 0, 0, 1] |   G | - | [0, 1, 0, 0] |[0, 1, 0, 0] |[0, 1, 0, 0] |\n","\" | G | R | [0, 1, 1, 0] |   R | - | [1, 0, 0, 0] |[1, 0, 0, 0] |[1, 0, 0, 0] |\n","\" | G | G | [0, 1, 0, 1] |   G | - | [0, 1, 0, 0] |[0, 1, 0, 0] |[0, 1, 0, 0] |\n","point color | R | R | [1, 0, 1, 0] |   - | R | [0, 0, 1, 0] |[0, 0, 1, 0] |[0, 0, 1, 0] |\n","\" | R | G | [1, 0, 0, 1] |   - | G | [0, 0, 0, 1] |[0, 0, 0, 1] |[0, 0, 0, 1] |\n","\" | G | R | [0, 1, 1, 0] |   - | R | [0, 0, 1, 0] |[0, 0, 1, 0] |[0, 0, 1, 0] |\n","\" | G | G | [0, 1, 0, 1] |   - | G | [0, 0, 0, 1] |[0, 0, 0, 1] |[0, 0, 0, 1] |\n"]},{"cell_type":"markdown","metadata":{"id":"T6ndBT4cqNbv"},"source":["# Formatting overview : data stored in a dictionary\n","\n","Use of this library depends on formatting of experimental data.\n","\n","* most data is broken down by \n","  * what is being measured (eg, behavior vs fmri)\n","  * what task (or set of tasks) is being performed\n","    * task sets are encoded by a string witn input/output dimensions separated by commas\n","    * example: '0,0,1,1' means 'point color and read word'\n","* since the numpy input code is the same for every task, we only input this data once, under keyword \"data\""]},{"cell_type":"code","metadata":{"id":"HAoxbK7oGEkz"},"source":["from importlib import reload\n","\n","import numpy as np\n","import aptfuns\n","reload(aptfuns)\n","from aptfuns import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TFXEL9z5Vdj"},"source":["#   define the dictionary\n","aptd              =   {}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rBGibrR35Vdk"},"source":["### size and number of dimensions"]},{"cell_type":"code","metadata":{"id":"lGwG24QCj5jx"},"source":["#   record the number of input + output dimensions\n","aptd['nid']       =   2 # number of input dimensions\n","aptd['nod']       =   2 # number of output dimensions\n","\n","#   record the number of input + output dimensions\n","aptd['id__nu']    =   [2,2] # number of distinct feature values in each input dimension\n","aptd['od__nu']    =   [2,2] # number of coordinates/entries in each output vector (THIS IS A LITTLE IRRELEVANT FOR ABBY)\n","aptd['id__u0']    =   np.concatenate(([0],np.cumsum(aptd['id__nu'])[:-1])) # this list records the location of the \"first unit\" in each input dimension\n","aptd['od__u0']    =   np.concatenate(([0],np.cumsum(aptd['od__nu'])[:-1])) # this list records the location of the \"first unit\" in each output dimension"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BilivxMb5Vdl"},"source":["### (sets of) tasks performed"]},{"cell_type":"markdown","metadata":{"id":"hILqU8JuIGEV"},"source":["A \"task\" is stored as a list of length 2: [a,b] represents the task sending input dimension a to output dimension b.  \n","\n","A \"perofrmance list\" is a list of tasks.  For example, [[a,b], [a',b']] means \"perform tasks [a,b] and [a',b'] simultaneously.\"  \n","\n","A \"performance list list\" is a list of performance lists.  Typically we only use this list for record keeping (so that we can remember which perofrmance lists we have and have not trained the network to perform).  The line below declares that we have data for (1) color pointing, (2) color speaking, (3) word pointing, (4) word speaking, (5) simultaneous color pointing and word speaking, and (6) simultaneous color speaking while word pointing."]},{"cell_type":"code","metadata":{"id":"r9XIjMkDIYCB"},"source":["aptd['pll']       =   [[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[0,0],[1,1]], [[0,1],[1,0]]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4tzwhQa_5Vdm"},"source":["### stimuli"]},{"cell_type":"markdown","metadata":{"id":"j4nW7JywLAFr"},"source":["We assume that the agent recieves the same set of input stimuli no matter what task is performed.  Moreover, we assume that stimuli are sorted lexicographically according to feature value. (**abby doesn't neet to input this one**)"]},{"cell_type":"code","metadata":{"id":"RKCOxHiuK0H-"},"source":["aptd['data']      =   np.array(   [ [1, 0, 1, 0],\n","                                    [1, 0, 0, 1],\n","                                    [0, 1, 1, 0],\n","                                    [0, 1, 0, 1]  ]\n","                                   )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V-XznjpH5Vdn"},"source":["### instructions, behavior, and activity patterns"]},{"cell_type":"markdown","metadata":{"id":"YRjtEYVrLj8-"},"source":["Record the activity patterns in the hidden layer for each performance list."]},{"cell_type":"code","metadata":{"id":"PUft2DbdLi4l"},"source":["aptd['fmri']          = {}\n","aptd['fmri']['0,0']   = np.array(   [   [1, 0, 0, 0],\n","                                        [1, 0, 0, 0],\n","                                        [0, 1, 0, 0],\n","                                        [0, 1, 0, 0]  ]\n","                                   )\n","\n","aptd['fmri']['0,1']   = np.array(   [   [1, 0, 0, 0],\n","                                        [1, 0, 0, 0],\n","                                        [0, 1, 0, 0],\n","                                        [0, 1, 0, 0]  ]\n","                                   )\n","\n","aptd['fmri']['1,0']   = np.array(   [   [0, 0, 1, 0],\n","                                        [0, 0, 0, 1],\n","                                        [0, 0, 1, 0],\n","                                        [0, 0, 0, 1]  ]\n","                                   )\n","\n","aptd['fmri']['1,1']   = np.array(   [   [0, 0, 1, 0],\n","                                        [0, 0, 0, 1],\n","                                        [0, 0, 1, 0],\n","                                        [0, 0, 0, 1]  ]\n","                                   )\n","\n","aptd['fmri']['0,0,1,1']   = np.array([  [1, 0, 1, 0],\n","                                        [1, 0, 0, 1],\n","                                        [0, 1, 1, 0],\n","                                        [0, 1, 0, 1]  ]\n","                                   )\n","\n","aptd['fmri']['0,1,1,0']   = np.array([  [1, 0, 0, 1],\n","                                        [0, 1, 0, 1],\n","                                        [1, 0, 1, 0],\n","                                        [0, 1, 1, 0]  ]\n","                                   )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJ5_Tk1VNHqR"},"source":["Record the activity patterns in the output layer for each performance list. "]},{"cell_type":"code","metadata":{"id":"nP7HRvdjOKCf"},"source":["aptd['behavior']          = {}\n","\n","aptd['behavior']['0,0']   = np.array([  [1, 0, 0, 0],\n","                                        [1, 0, 0, 0],\n","                                        [0, 1, 0, 0],\n","                                        [0, 1, 0, 0]  ]\n","                                  )\n","\n","aptd['behavior']['0,1']   = np.array([  [0, 0, 1, 0],\n","                                        [0, 0, 1, 0],\n","                                        [0, 0, 0, 1],\n","                                        [0, 0, 0, 1]  ]\n","                                  )\n","\n","aptd['behavior']['1,0']   = np.array([  [1, 0, 0, 0],\n","                                        [0, 1, 0, 0],\n","                                        [1, 0, 0, 0],\n","                                        [0, 1, 0, 0]  ]\n","                                  )\n","\n","aptd['behavior']['1,1']   = np.array([  [0, 0, 1, 0],\n","                                        [0, 0, 0, 1],\n","                                        [0, 0, 1, 0],\n","                                        [0, 0, 0, 1]  ]\n","                                  )\n","\n","\n","aptd['behavior']['0,0,1,1']= np.array([ [1, 0, 1, 0],\n","                                        [1, 1, 1, 1],\n","                                        [1, 1, 1, 1],\n","                                        [0, 1, 0, 1]  ]\n","                                   )\n","\n","aptd['behavior']['0,1,1,0']= np.array([ [1, 0, 1, 0],\n","                                        [1, 1, 1, 1],\n","                                        [1, 1, 1, 1],\n","                                        [0, 1, 0, 1]  ]\n","                                   )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYFrAuDnNa9W"},"source":["Record the labels for each performance list.\n"]},{"cell_type":"code","metadata":{"id":"5nF-k137NX4X"},"source":["aptd['instruction']          = {}\n","\n","aptd['instruction']['0,0']   = np.array([ [1, 0, 0, 0],\n","                                          [1, 0, 0, 0],\n","                                          [0, 1, 0, 0],\n","                                          [0, 1, 0, 0]  ]\n","                                   )\n","\n","aptd['instruction']['0,1']   = np.array([ [0, 0, 1, 0],\n","                                          [0, 0, 1, 0],\n","                                          [0, 0, 0, 1],\n","                                          [0, 0, 0, 1]  ]\n","                                   )\n","\n","aptd['instruction']['1,0']   = np.array([ [1, 0, 0, 0],\n","                                          [0, 1, 0, 0],\n","                                          [1, 0, 0, 0],\n","                                          [0, 1, 0, 0]  ]\n","                                   )\n","\n","aptd['instruction']['1,1']   = np.array([ [0, 0, 1, 0],\n","                                          [0, 0, 0, 1],\n","                                          [0, 0, 1, 0],\n","                                          [0, 0, 0, 1]  ]\n","                                   )\n","\n","aptd['instruction']['0,0,1,1']=np.array([ [1, 0, 1, 0],\n","                                          [1, 0, 0, 1],\n","                                          [0, 1, 1, 0],\n","                                          [0, 1, 0, 1]  ]\n","                                   )\n","\n","aptd['instruction']['0,1,1,0']=np.array([ [1, 0, 1, 0],\n","                                          [0, 1, 1, 0],\n","                                          [1, 0, 0, 1],\n","                                          [0, 1, 0, 1]  ]\n","                                   )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0LyxeRJ95Vds"},"source":["# Application: compare representations"]},{"cell_type":"markdown","metadata":{"id":"fS3vs_Hq5Vdt"},"source":["### simple mean representations\n","\n","*What does simple mean representation mean?*  The \"mean representation\" of a task is the mean hidden activation pattern, averaging over all stimuli."]},{"cell_type":"code","metadata":{"id":"DK0gVX6J5Vdt","outputId":"29a4fe2c-5d59-490a-a909-e21f422405dd"},"source":["# compare two tasks which both use input dimension 0\n","aptfuns.aptd_ln_pl0_pl1_distofmeanreps(aptd,'fmri',[[0,0]],[[0,1]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":274}]},{"cell_type":"code","metadata":{"id":"crMP_btt5Vdv","outputId":"2a3b498f-7588-43e3-9def-5d1362620469"},"source":["# compare two tasks using two different input dimensions\n","aptfuns.aptd_ln_pl0_pl1_distofmeanreps(aptd,'fmri',[[0,0]],[[1,1]])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":275}]},{"cell_type":"markdown","metadata":{"id":"OgWFWgnS5Vdv"},"source":["### featurewise-centered representations\n","\n","*What does featurewise-centered representation mean?* Fix an input dimension (say 0).  Each tuple of feature values in the extraneous input dimension(s) then determines a map \n","\n","$$ \\{\\text{feature values in the relevant input dim} \\} \\longrightarrow \\{\\text{hidden activation patterns}\\} $$\n","\n","The *featurewise centered* representation is obtained by adding a constant offset to each map so that its values center around zero.  The representation itself is indexed family of all these feature-wise centered maps (indexed, that is, by tuples of feature values in the irrelevant dimensions)."]},{"cell_type":"code","metadata":{"id":"DIfGe5KI5Vdw","outputId":"313918f7-0047-4eb1-e309-cf03794ac763"},"source":["# while the network is multitasking\n","aptfuns.aptd_ln_pl0_pl1_id__featurewisecenterrepdist(aptd, 'fmri', [[0,0],[1,1]], [[0,1],[1,0]], 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.0"]},"metadata":{"tags":[]},"execution_count":276}]},{"cell_type":"code","metadata":{"id":"uqFhRJ4H5Vdw","outputId":"a1ca45c4-e611-4233-b5ac-5cf143535d10"},"source":["# while the network is single-tasking\n","aptfuns.aptd_ln_pl0_pl1_id__featurewisecenterrepdist(aptd, 'fmri', [[0,0]], [[0,1]], 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":277}]},{"cell_type":"markdown","metadata":{"id":"GUdK5zcD5Vdx"},"source":["### beta representations\n","\n","*What does 'beta representation' mean?*  The (the beta values of the) linear regressors with\n","\n","* **depenent variables** = hidden activation patterns \n","* **independent variables** = the feature value of input dimension 0 and \n","* **controling for** the features in all other input dimensions"]},{"cell_type":"code","metadata":{"id":"znAxlRbe5Vdx","outputId":"1440a7a7-7593-4d70-b1fd-338507d8c8e1"},"source":["# while the network is multitasking\n","aptfuns.aptd_ln_pl0_pl1_id__betarepdist(aptd, 'fmri', [[0,0],[1,1]], [[0,1],[1,0]], 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.4142135623730951"]},"metadata":{"tags":[]},"execution_count":278}]},{"cell_type":"code","metadata":{"id":"U1bGFCMz5Vdz","outputId":"ceb8ecda-725e-4347-e273-a69722026110"},"source":["# while the network is single-tasking\n","aptfuns.aptd_ln_pl0_pl1_id__betarepdist(aptd, 'fmri', [[0,0]], [[0,1]], 0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":279}]},{"cell_type":"code","metadata":{"id":"9JberxtQ5Vdz","outputId":"99f7efe0-a3db-4d50-a316-655e82452827"},"source":["# CHECK THAT THE FIRST NUMBER MAKES SENSE (IT DOES)\n","print(aptfuns.aptd_ln_pl_id__betarep(aptd, 'fmri', [[0,0],[1,1]], 0))\n","print(aptfuns.aptd_ln_pl_id__betarep(aptd, 'fmri', [[0,1],[1,0]], 0))\n","print(2**(1/2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0.5 -0.5  0.   0. ]\n"," [-0.5  0.5  0.   0. ]]\n","[[ 0.   0.  -0.5  0.5]\n"," [ 0.   0.   0.5 -0.5]]\n","1.4142135623730951\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HLpMQT965Vd0"},"source":["# compute interaction effects"]},{"cell_type":"code","metadata":{"id":"w8qWcuFi5Vd1","outputId":"ab92aa5a-533b-4074-a363-0341f8cc35e2"},"source":["# aggregate interaction effect between two given input dimensions while mapping two given output dimensions\n","\n","np.mean(aptfuns.aptd_ln_id01_od01_plc__iena(aptd, 'fmri', 0, 1, 0, 1, 2))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.0"]},"metadata":{"tags":[]},"execution_count":281}]},{"cell_type":"code","metadata":{"id":"QZiBlCmr5Vd1","outputId":"7c760151-3fc9-4c78-af61-844a24399085"},"source":["# aggregate interaction effect between two given input dimensions while mapping two given output dimensions\n","\n","np.mean(aptfuns.aptd_ln_id01_od01_plc__iena(aptd, 'fmri', 0, 1, 0, 1, 1))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0"]},"metadata":{"tags":[]},"execution_count":282}]}]}